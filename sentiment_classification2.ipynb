{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is based on the available kaggle competition - Detection of toxic comment.  \n",
    "link: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import necessary packages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import string\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import Imputer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=pd.read_csv('datasets/tow_classes_tweet.csv', encoding='utf-8', usecols=['sentiment', 'txt'])\n",
    "dataset1=pd.read_csv('C:/Users/aas3n17/Desktop/Organized/datasets/Ar_tweet_cleaned_final.csv', encoding='utf-8', usecols=['sentiment', 'txt'])\n",
    "dataset2=pd.read_csv('C:/Users/aas3n17/Desktop/Organized/datasets/qrci.csv', encoding='utf-8', usecols=['sentiment', 'txt'])\n",
    "dataset3=pd.read_csv('C:/Users/aas3n17/Desktop/Organized/datasets/astd.csv', encoding='utf-8', usecols=['sentiment', 'txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    993\n",
       "0    958\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    377\n",
       "0    377\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    812\n",
       "1    777\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=clean(dataset1)\n",
    "dataset2=clean(dataset2)\n",
    "dataset3=clean(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     عن اي مشروب غازي كوﻻ حافظمشفاهم  \n",
       "1     عاده لا اعلن عن مواقفي السياسيهلكن كله الا الد...\n",
       "2     انتا فرحه جات ل عندي بعد دنيا من التعب السعاده...\n",
       "3     البلاوي تصنيف الاخوان تنظيما ارهابيا قرار سياس...\n",
       "4      فكره المقاله حلوه واسلوبك في توصيل الفكره حلو...\n",
       "5     اهالي عزبهالنخل يفضحون السيسي حملته قالت لنا ا...\n",
       "6     الساده الافاضل بجريده المصري اليوم عن الخبر ال...\n",
       "7     الحمد له ان حقن دماء المصرين واصلح بينهم وعقبا...\n",
       "8     يا مرسي يا عظيم تسريب سيسي كشف انه كان عملاقا ...\n",
       "9     العبيد في اعلام العار لم ينطق بعد تصريح ايهاب ...\n",
       "10    اسرار نشاه السيسي داخل حاره اليهود بالقاهره ال...\n",
       "11    لبيك الهم لبيك لبيك لا شريك لك لبيك ان الحمد و...\n",
       "12    نحن امه اذا وضعت لمستخدمي الانترنت فيها كلمه ف...\n",
       "13    الهم امطر علي قبر محمد حسين جمعه وعلي قبور جمي...\n",
       "14    حفيد السلطان عبدالحميد مرشح لبرلمان التركي الق...\n",
       "15    من اجمل ذكريات الطفوله انك تنام في اي مكان في ...\n",
       "16    المصري بيقول حزام ناسف و الشروق بيقول سياره مس...\n",
       "17    ع منطق حقير ولا يتبعه سوي الجبناء هذه نفس تقتل...\n",
       "18    بعد مصادره فيلم حلاوه روح منع سمالمصري من الغن...\n",
       "19                                         شكرا لدعمك  \n",
       "20    استفتاء مارس مانفعش طنطاوي وقد كان اكثر شرفا و...\n",
       "21    صاحب قناه الفراعين لمصري اليوم انا ملك التوك ش...\n",
       "22    نعم انه خالدبنالوليد رضي اله عنه يا من يفتخر ب...\n",
       "23    رحم اله الاستاذ سلامه احمد سلامه، قلم الضمير و...\n",
       "24    اعتذار سي بي سي عن حلقه الشيطان الاراجوز جزء م...\n",
       "25     تدعون لدين وتنشرون الاكاذيب، لم اكن ابدا عضوا...\n",
       "26    كانوا رجال مرسيرءيسي كانوارجال الشعبيردتحيهالر...\n",
       "27    الي حمدينصباحي لو انت عاوز تحاكم السيسي يبقي ا...\n",
       "28    القاكم بعد ربع ساعه من الان في برنامج الحياه ا...\n",
       "29    ارجو الدعاء لراحل العظيم محمود السعدني اليوم ذ...\n",
       "                            ...                        \n",
       "70    واحد بيطبل بقاله سنين وفي الاخر مش عايزينه يبق...\n",
       "71                                      اشكرك يا خالد  \n",
       "72    ب وبعد عزل اجباري لرءيس المدني المنتخب كنت اتم...\n",
       "73    اتمت مهمتي كعضو لجنه تحكيم في مهرجان فينيسيا ا...\n",
       "74    وجه نظر ساذجه تقول ان من ناضل في الماضي يحق له...\n",
       "75    سياده الرءيس، انا احترمك بقدر احترامك لمحكمه ا...\n",
       "76    كنت تاجل وتيجي منك بجميله وتعمل فيها قعر مجلس ...\n",
       "77    ستظل ذكري استشهاد البطل عبد المنعم رياض تخليدا...\n",
       "78    لم اتحدث مع ال خلال الايام الماضيه علي الاطلاق...\n",
       "79     السباحه ازاي السياح يجوا و محافظ الاقصر ارهابي  \n",
       "80                                       انت بضان ياض  \n",
       "81    صدق من قال لاتستطيع شراء الاخوان لانهم قوم لاس...\n",
       "82    بلطجه الاخوان والاعتداء علي الاعلامين بمحيط اك...\n",
       "83                  مصر اليوم حره بارك اله في شعب مصر  \n",
       "84    ماعنديش فلتر بين مخي ولساني دي مصيبه ال بفكر ف...\n",
       "85      لمصر شعب يحميها رب اجعل هذا البلد امنا مطمءنا  \n",
       "86    نحب الغه العربيه لانها لغه القران ولان نبينا ع...\n",
       "87    شفت كل شي بالحياه بس اول مره اشوف شب حامل طبله...\n",
       "88    مفاجاه رد الشيخ حازم علي فتوي برهامي الاخيره ف...\n",
       "89     نبضالاخوان ا عصامسلطان و ثبات علي المبدا احنا...\n",
       "90    بمناسبه عيد تحرير سينا السيد الرءيس الساده صنا...\n",
       "91                                     وبحبك يا زمالك  \n",
       "92    الماظهلابد من معايره العدادات وتوحيد التعريفه ...\n",
       "93    انا لو نفسي اخط و ارتب و اصرف فلوس الدنيا عشان...\n",
       "94    نزلنا نايدك ومفيش حاجه اتغيرت وبقينا مسخه لعال...\n",
       "95                            فريق الوعد انصافالاخوان  \n",
       "96    اول مره اشوف مناظره بدون مقاطعه ولا صراخ وكل ط...\n",
       "97    يوما ما سيدرس اولادنا شجاعه المراه المصريه الت...\n",
       "98    اري ان النظام المختلط بين النظام البرلماني وال...\n",
       "99    سبحان اله والحمد له ولا اله الا اله ولا حول ول...\n",
       "Name: txt, Length: 100, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3.txt.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.1)\n",
    "train.head()\n",
    "#prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little preprocessing required\n",
    "sentences_train = train[\"txt\"].fillna(\"_na_\").values\n",
    "classes = [\"sentiment\"]\n",
    "y = train[classes].values\n",
    "sentences_test = test[\"txt\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding parameter set\n",
    "embed_size = 100 # how big is each word vector\n",
    "max_features = 10000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(sentences_train))\n",
    "tokens_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "tokens_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "X_train = pad_sequences(tokens_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(tokens_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 50, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 4)             1680      \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 47, 16)            272       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 752)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               75300     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,077,353\n",
      "Trainable params: 1,077,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(4, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)(x)\n",
    "x = Conv1D(16,4,activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(100, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.rmsprop(lr = 0.001,decay = 1e-06), metrics=['accuracy'])\n",
    "filepath=\"Weights/weights-improvement.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the defined model onto the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1404 samples, validate on 351 samples\n",
      "Epoch 1/5\n",
      "1404/1404 [==============================] - 3s 2ms/step - loss: 0.6678 - acc: 0.5848 - val_loss: 0.6238 - val_acc: 0.5897\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58974, saving model to Weights/weights-improvement.hdf5\n",
      "Epoch 2/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.4524 - acc: 0.8027 - val_loss: 0.4049 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58974 to 0.81197, saving model to Weights/weights-improvement.hdf5\n",
      "Epoch 3/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.2513 - acc: 0.9010 - val_loss: 0.3310 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81197 to 0.85470, saving model to Weights/weights-improvement.hdf5\n",
      "Epoch 4/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.1458 - acc: 0.9459 - val_loss: 0.3378 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.85470\n",
      "Epoch 5/5\n",
      "1404/1404 [==============================] - 2s 1ms/step - loss: 0.0833 - acc: 0.9708 - val_loss: 0.3920 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85470 to 0.85755, saving model to Weights/weights-improvement.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ecd1d33b38>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y, batch_size=32, epochs=5,callbacks=callbacks_list, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select from saved weights as per choice and predict response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "saved_model = load_model('Weights/weights-improvement.hdf5')\n",
    "y_pred = saved_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=y_pred.round()\n",
    "y_test=test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8342494714587738, 0.813953488372093, 0.8235294117647058, 0.8045977011494253]\n"
     ]
    }
   ],
   "source": [
    "        f1_score = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "        # macro accuracy (macro average)\n",
    "        macc = metrics.f1_score(y_test, y_pred, pos_label=None, average='macro')\n",
    "\n",
    "        # precision and recall\n",
    "        recall = metrics.recall_score(y_test, y_pred)\n",
    "        precision = metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "        results = [macc, f1_score, precision, recall]\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def clean(df):\n",
    "        path = \"Arabic_stop_word.txt\"\n",
    "        stop_words = []\n",
    "        with codecs.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n",
    "            stop_words = myfile.readlines()\n",
    "        stop_words = [word.strip() for word in stop_words]\n",
    "        arabic_punctuations = '''؛<>_()*،&^%][ـ،/:\"؟ـ`÷×.,'{}~¦+|!”…“–_'''\n",
    "        arabic_numbers = '''۰۱۲۳٤٥٦٧۸۹'''\n",
    "        english_punctuations = string.punctuation\n",
    "        punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "        arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "        for comments in df:\n",
    "            # removearabic_diacritics\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(arabic_diacritics,'',str(x)))\n",
    "\n",
    "            #def normalize_arabic(text)\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"[إأآا]\", \"ا\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ؤ\", \"ء\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ى\", \"ي\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ئ\", \"ء\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ة\", \"ه\",str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"گ\", \"ك\",str(x)))\n",
    "\n",
    "            \n",
    "\n",
    "            #def remove_punctuations(text):\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"[\"+punctuations_list+\"]\",'',str(x)))\n",
    "\n",
    "            # remove_repeating_char(text):\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(r'(.)\\1+', r'\\1', str(x)))\n",
    "            # remove '\\\\n'\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "\n",
    "            # remove any text starting with User... \n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n",
    "\n",
    "            # remove IP addresses or user IDs\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n",
    "\n",
    "            # lower uppercase letters\n",
    "            #df['txt'] = df['txt'].map(lambda x: str(x).lower())\n",
    "\n",
    "            #remove http links in the text\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n",
    "\n",
    "            #remove all punctuation\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"_\", '',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"«\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"»\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"“\", ' ',str(x))) \n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"”\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"😞\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"😔\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"😂\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"🌹\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"✨\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"👉\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"👈\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"☹\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"👇\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ک\", 'ك',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"—\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"ლ\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"╹\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"◡\", ' ',str(x)))\n",
    "            \n",
    "            \n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"♥\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"♡\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"¥\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"؟\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"!\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"#\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"$\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"%\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"&\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"✖\", ' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('ة', 'ه',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(' ئ', 'ء',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('ؤ', 'ء',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('ے', 'ك',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('ڪ', 'ك',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('أ', 'ا',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('إ', 'ا',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('آ', 'ا',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub('\\\\r',' ',str(x)))\n",
    "            df['txt']= df['txt'].map(lambda x: re.sub(\"[\"+string.punctuation+\"]\",'',str(x)))\n",
    "            df['txt']= df['txt'].map(lambda x: re.sub(\"[\"+arabic_numbers+\"]\",'',str(x)))\n",
    "            df['txt']= df['txt'].map(lambda x: re.sub(\"[\"+punctuations_list+\"]\",'',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(\"[\"+digits+\"]\",'',str(x)))\n",
    "            #df['txt'] = df['txt'].map(lambda x: re.sub(r'[^\\x600-\\x6ff]','',str(x)))\n",
    "            df['txt'] = df['txt'].map(lambda x: re.sub(r'[a-zA-Z?]','',str(x))) \n",
    "            df['txt'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
